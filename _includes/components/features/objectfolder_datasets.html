<div
  class="section features-3 my-5 fullWidth"
  style="background-image: url(' {{ '/assets/img/ill/p31.svg' | relative_url }}')">
  <div class="container">
    <div class="row text-center justify-content-center">
      <div class="col-lg-8">
        <h3 class="display-3 text-white">ObjectFolder Datasets</h3>
        <br /><br />
      </div>
    </div>
    <div class="row justify-content-center align-items-center">
      <div class="col">
        <div class="info info-horizontal text-white">
          <div class="description pl-4">
            <h4 class="title text-warning">ObjectFolder-Real</h4>

            <p class="description opacity-8">
              <li>100 real-world household objects.</li>
              <li>A tailored data collection pipeline for each modality.</li>
              <li>
                For vision, we scan the 3D meshes of objects in a dark room and
                record HD videos of each object rotating in a lightbox.
              </li>
              <li>
                For audio, we build a professional anechoic chamber with a
                tailored object platform and then collect impact sounds by
                striking the objects at different surface locations with an
                impact hammer.
              </li>
              <li>
                For touch, we equip a Franka Emika Panda robot arm with a
                GelSight robotic finger and collect tactile readings at the
                exact surface locations where impact sounds are collected.
              </li>
            </p>
            <a href="https://www.objectfolder.org/objectfolder-real" class="text-warning">Explore ObjectFolder-Real</a>
          </div>
        </div>
      </div>

      <div class="col">
        <img
          class="ml-lg-5"
          src="{{ '/assets/img/objectfolder/ObjectFolder-Real.jpg' | relative_url }}"
          width="450px" />
      </div>

      <div class="w-100"></div>
      <br />
      <br />

      <div class="col">
        <div class="info info-horizontal text-white">
          <div class="description pl-4">
            <h4 class="title text-info">ObjectFolder 2.0</h4>
            <p class="description opacity-8">
              <li>1,000 multisensory neural objects.</li>
              <li>
                Each neural object is represented by an Object File, a compact
                neural network that encodes the object's intrinsic visual,
                acoustic, and tactile sensory data.
              </li>
              <li>
                Querying the Object File with extrinsic parameters (e.g., camera
                viewpoint and lighting conditions for vision, impact location
                and strength for audio, contact location and gel deformation for
                touch), we can obtain the corresponding sensory signal at a
                particular location or condition.
              </li>
            </p>
            <a
              href="https://ai.stanford.edu/~rhgao/objectfolder2.0/"
              class="text-info"
              >Explore ObjectFolder 2.0</a
            >
          </div>
        </div>
      </div>
      <div class="col">
        <img
          class="ml-lg-5"
          src="{{ '/assets/img/objectfolder/ObjectFolder2.png' | relative_url }}"
          width="450px" />
      </div>
    </div>
  </div>
</div>
